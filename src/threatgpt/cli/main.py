"""Main CLI entry point for ThreatGPT."""

import sys
import os
import logging
from pathlib import Path
from typing import Optional
from datetime import datetime

import click
from rich.console import Console
from rich.table import Table
from dotenv import load_dotenv

from threatgpt import __version__
from .templates import templates
from .llm import llm_group
from .logs import logs as logs_group

# Load environment variables from .env file
load_dotenv()

console = Console()


@click.group()
@click.version_option(version=__version__, prog_name="ThreatGPT")
@click.option(
    "--verbose", "-v", is_flag=True, help="Enable verbose output"
)
@click.option(
    "--config",
    "-c",
    default="~/.threatgpt/config.yaml",
    help="Configuration file path",
)
@click.pass_context
def cli(ctx: click.Context, verbose: bool, config: str) -> None:
    """ThreatGPT: AI-Powered Threat Simulation Platform.
    
    A production-grade cybersecurity threat simulation platform that leverages
    Large Language Models (LLMs) to generate realistic, context-aware threat
    scenarios for training, awareness, and red teaming activities.
    """
    ctx.ensure_object(dict)
    ctx.obj["verbose"] = verbose
    ctx.obj["config"] = config
    
    if verbose:
        console.print(f"[green]ThreatGPT v{__version__}[/green]")
        console.print(f"[dim]Config: {config}[/dim]")


@cli.command()
@click.option(
    "--scenario",
    "-s",
    required=True,
    help="Path to threat scenario configuration file",
)
@click.option(
    "--target",
    "-t",
    help="Target profile specification",
)
@click.option(
    "--output",
    "-o",
    type=click.Choice(["json", "yaml", "report"]),
    default="report",
    help="Output format",
)
@click.option(
    "--dry-run",
    is_flag=True,
    help="Validate configuration without executing simulation",
)
@click.option(
    "--preview",
    is_flag=True,
    help="Show detailed scenario preview before execution",
)
@click.pass_context
def simulate(
    ctx: click.Context,
    scenario: str,
    target: Optional[str],
    output: str,
    dry_run: bool,
    preview: bool,
) -> None:
    """Execute a threat simulation scenario."""
    from ..config.yaml_loader import YAMLConfigLoader, ConfigurationError, SchemaValidationError
    from rich.panel import Panel
    from rich.table import Table
    from datetime import datetime
    import json
    
    scenario_path = Path(scenario)
    console.print(f"[blue]Loading threat scenario:[/blue] {scenario_path.name}")
    
    # Load and validate the scenario
    loader = YAMLConfigLoader()
    try:
        scenario_config = loader.load_and_validate_scenario(scenario_path)
        console.print("[green]Scenario loaded and validated successfully[/green]")
    except (ConfigurationError, SchemaValidationError) as e:
        console.print(f"[red]Scenario validation failed:[/red] {e}")
        return
    
    # Display scenario summary
    metadata = scenario_config.metadata
    threat_type_val = scenario_config.threat_type.value if hasattr(scenario_config.threat_type, 'value') else str(scenario_config.threat_type)
    delivery_vector_val = scenario_config.delivery_vector.value if hasattr(scenario_config.delivery_vector, 'value') else str(scenario_config.delivery_vector)
    difficulty_val = scenario_config.difficulty_level.value if hasattr(scenario_config.difficulty_level, 'value') else scenario_config.difficulty_level
    
    summary_text = f"[bold cyan]{metadata.name}[/bold cyan]\n"
    summary_text += f"{metadata.description}\n\n"
    summary_text += f"[bold]Threat Type:[/bold] {threat_type_val.replace('_', ' ').title()}\n"
    summary_text += f"[bold]Delivery Vector:[/bold] {delivery_vector_val.replace('_', ' ').title()}\n"
    summary_text += f"[bold]Difficulty:[/bold] {difficulty_val}/10\n"
    summary_text += f"[bold]Duration:[/bold] {scenario_config.estimated_duration} minutes"
    
    console.print(Panel(summary_text, title="Scenario Overview", border_style="cyan"))
    
    # Show target profile
    target_profile = scenario_config.target_profile
    seniority_val = target_profile.seniority.value if hasattr(target_profile.seniority, 'value') else str(target_profile.seniority)
    technical_val = target_profile.technical_level.value if hasattr(target_profile.technical_level, 'value') else str(target_profile.technical_level)
    
    target_text = f"[bold]Role:[/bold] {target_profile.role}\n"
    target_text += f"[bold]Department:[/bold] {target_profile.department}\n"
    target_text += f"[bold]Seniority:[/bold] {seniority_val.replace('_', ' ').title()}\n"
    target_text += f"[bold]Technical Level:[/bold] {technical_val.title()}\n"
    target_text += f"[bold]Security Awareness:[/bold] {target_profile.security_awareness_level}/10"
    
    if target_profile.industry:
        industry_val = target_profile.industry.value if hasattr(target_profile.industry, 'value') else str(target_profile.industry)
        target_text += f"\n[bold]Industry:[/bold] {industry_val.replace('_', ' ').title()}"
    
    console.print(Panel(target_text, title="Target Profile", border_style="green"))
    
    # Show MITRE ATT&CK techniques if available
    behavioral_pattern = scenario_config.behavioral_pattern
    if behavioral_pattern.mitre_attack_techniques:
        techniques_text = ", ".join(behavioral_pattern.mitre_attack_techniques)
        console.print(Panel(f"{techniques_text}", title="MITRE ATT&CK Techniques", border_style="red"))
    
    if dry_run:
        console.print("[yellow]Dry run mode - configuration validation complete[/yellow]")
        console.print("[green]Scenario is ready for execution[/green]")
        return
    
    if preview or (ctx.obj and ctx.obj.get("verbose", False)):
        # Show detailed execution plan
        console.print("\n[bold cyan]Execution Plan:[/bold cyan]")
        
        execution_table = Table(show_header=True, header_style="bold magenta")
        execution_table.add_column("Phase", style="cyan")
        execution_table.add_column("Action", style="white")
        execution_table.add_column("Details", style="dim")
        
        # Simulate execution phases based on scenario type
        if "phishing" in threat_type_val.lower():
            execution_table.add_row("1. Reconnaissance", "Target Research", "Gather information about target and organization")
            execution_table.add_row("2. Content Generation", "Email Creation", "Generate personalized phishing email using LLM")
            execution_table.add_row("3. Delivery Setup", "Infrastructure", "Set up landing pages and tracking")
            execution_table.add_row("4. Simulation Launch", "Send Email", "Deliver phishing email to target")
            execution_table.add_row("5. Monitoring", "Track Responses", "Monitor clicks, responses, and behavior")
        elif "social_engineering" in threat_type_val.lower():
            execution_table.add_row("1. Target Analysis", "Profile Study", "Analyze target's role and behavior patterns")
            execution_table.add_row("2. Pretext Development", "Scenario Creation", "Develop convincing pretext using LLM")
            execution_table.add_row("3. Contact Preparation", "Script Generation", "Generate conversation scripts and responses")
            execution_table.add_row("4. Engagement", "Initiate Contact", "Begin social engineering interaction")
            execution_table.add_row("5. Documentation", "Record Results", "Log responses and effectiveness")
        else:
            execution_table.add_row("1. Preparation", "Scenario Setup", "Initialize simulation environment")
            execution_table.add_row("2. Content Generation", "LLM Processing", "Generate threat content using AI")
            execution_table.add_row("3. Delivery", "Execute Attack", "Deliver simulation to target")
            execution_table.add_row("4. Monitoring", "Track Progress", "Monitor simulation progress")
            execution_table.add_row("5. Analysis", "Collect Results", "Generate results and metrics")
        
        console.print(execution_table)
        
        # Show simulation parameters
        sim_params = scenario_config.simulation_parameters
        params_text = f"[bold]Max Iterations:[/bold] {sim_params.max_iterations}\n"
        params_text += f"[bold]Max Duration:[/bold] {sim_params.max_duration_minutes} minutes\n"
        params_text += f"[bold]Escalation:[/bold] {'Enabled' if sim_params.escalation_enabled else 'Disabled'}\n"
        params_text += f"[bold]Adaptation:[/bold] {'Enabled' if sim_params.response_adaptation else 'Disabled'}\n"
        params_text += f"[bold]Language:[/bold] {sim_params.language}\n"
        params_text += f"[bold]Tone:[/bold] {sim_params.tone.replace('_', ' ').title()}\n"
        params_text += f"[bold]Urgency Level:[/bold] {sim_params.urgency_level}/10"
        
        console.print(Panel(params_text, title="Simulation Parameters", border_style="blue"))
    
    # Check if we should proceed with simulation
    if not preview and not click.confirm("\nProceed with simulation execution?"):
        console.print("[yellow]Simulation cancelled by user[/yellow]")
        return
    
    # Execute actual threat simulation
    console.print("\n[bold green]Starting Threat Simulation...[/bold green]")
    
    try:
        # Initialize simulation components
        from ..core.simulator import ThreatSimulator
        from ..core.models import ThreatScenario as CoreThreatScenario
        from ..llm.manager import LLMManager
        from ..core.simulation_logger import SimulationLogger
        from ..core.output_models import (
            SimulationOutput, ScenarioMetadata, TargetProfile, 
            SimulationMetrics, QualityAssessment, ContentGeneration,
            ProviderInfo, ContentType, SimulationStatus, OutputFormat
        )
        from ..utils.auto_content_saver import save_content_automatically
        
        # Convert YAML scenario to core model
        core_scenario = CoreThreatScenario.from_yaml_config(scenario_config)
        
        # Initialize LLM manager with environment-based configuration
        llm_config = {}
        
        # Configure OpenAI if available
        if os.getenv('OPENAI_API_KEY'):
            llm_config['openai'] = {
                'api_key': os.getenv('OPENAI_API_KEY'),
                'model': 'gpt-3.5-turbo'
            }
        
        # Configure Anthropic if available
        if os.getenv('ANTHROPIC_API_KEY'):
            llm_config['anthropic'] = {
                'api_key': os.getenv('ANTHROPIC_API_KEY'),
                'model': 'claude-3-haiku-20240307'
            }
        
        # Configure OpenRouter if available
        if os.getenv('OPENROUTER_API_KEY'):
            llm_config['openrouter'] = {
                'api_key': os.getenv('OPENROUTER_API_KEY'),
                'model': 'qwen/qwen3-vl-235b-a22b-thinking'  # Use working Qwen model
            }
        
        llm_manager = LLMManager(config=llm_config)
        
        # Check LLM provider status and test connection
        if not llm_manager.is_available():
            console.print("[red]No LLM provider configured. Simulation will use fallback content only.[/red]")
        else:
            # Test connection to verify real AI responses
            console.print("[blue]Testing LLM provider connection...[/blue]")
            
            import asyncio
            test_result = asyncio.run(llm_manager.test_connection())
            
            if test_result["status"] == "success":
                is_real_ai = test_result.get("is_real_ai", False)
                if is_real_ai:
                    console.print(f"[green]Real AI connection verified with {test_result['provider']}[/green]")
                else:
                    console.print(f"[yellow]{test_result['provider']} using mock/simulated responses[/yellow]")
            else:
                console.print(f"[red]LLM connection failed: {test_result.get('error', 'Unknown error')}[/red]")
                console.print("[yellow]Simulation will use fallback content.[/yellow]")
        
        # Initialize simulator
        sim_params = scenario_config.simulation_parameters
        simulator = ThreatSimulator(llm_provider=llm_manager, max_stages=sim_params.max_iterations)
        
        # Execute simulation
        console.print(f"[cyan]Executing scenario:[/cyan] {metadata.name}")
        console.print(f"[cyan]Threat type:[/cyan] {threat_type_val}")
        
        # Use asyncio to run the simulation
        import asyncio
        simulation_result = asyncio.run(simulator.execute_simulation(core_scenario))
        
        # Display results
        console.print(f"\n[bold green]Simulation completed![/bold green]")
        console.print(f"[cyan]Simulation ID:[/cyan] {simulation_result.result_id}")
        console.print(f"[cyan]Duration:[/cyan] {simulation_result.total_duration_seconds:.1f} seconds")
        console.print(f"[cyan]Success Rate:[/cyan] {simulation_result.success_rate:.1%}")
        console.print(f"[cyan]Stages Completed:[/cyan] {len(simulation_result.stages)}")
        
        # Create validated simulation output
        simulation_output = SimulationOutput(
            simulation_id=simulation_result.result_id,
            status=SimulationStatus.COMPLETED if simulation_result.status.value == "completed" else SimulationStatus.FAILED,
            success=simulation_result.status.value == "completed",
            started_at=simulation_result.start_time,
            completed_at=simulation_result.end_time or datetime.utcnow(),
            scenario=ScenarioMetadata(
                name=metadata.name,
                description=metadata.description,
                threat_type=threat_type_val,
                delivery_vector=delivery_vector_val,
                difficulty_level=getattr(scenario_config, 'difficulty_level', 5),
                mitre_techniques=getattr(scenario_config.behavioral_pattern, 'mitre_attack_techniques', []) if hasattr(scenario_config, 'behavioral_pattern') else [],
                scenario_file=str(scenario)
            ),
            target_profile=TargetProfile(
                role=target_profile.role,
                department=target_profile.department,
                seniority=seniority_val,
                industry=getattr(target_profile, 'industry', 'technology'),
                security_awareness=getattr(target_profile, 'security_awareness_level', 5)
            ),
            metrics=SimulationMetrics(
                success_rate=simulation_result.success_rate * 100,  # Convert to percentage
                stages_completed=len(simulation_result.stages),
                total_stages=len(simulation_result.stages),
                duration_seconds=simulation_result.total_duration_seconds
            ),
            quality_assessment=QualityAssessment(
                safety_compliance=True,
                content_appropriateness=True,
                detection_indicators=["Educational simulation markers included"]
            ),
            recommendations=[
                "Review simulation results and identify learning opportunities",
                "Consider implementing detected security gaps",
                "Plan follow-up training based on scenario outcomes"
            ],
            environment={
                "llm_provider": "openrouter" if "openrouter" in llm_manager.get_available_providers() else "fallback",
                "python_version": sys.version.split()[0],
                "threatgpt_version": __version__
            }
        )
        
        # Add generated content from stages
        for stage in simulation_result.stages:
            if hasattr(stage, 'content') and stage.content:
                content_gen = ContentGeneration(
                    content_type=ContentType.EMAIL,  # Default type
                    content=stage.content,
                    prompt_used=f"{stage.stage_type} generation for {threat_type_val}",
                    provider_info=ProviderInfo(
                        name=llm_manager.get_available_providers()[0] if llm_manager.get_available_providers() else "fallback",
                        model="unknown"
                    )
                )
                simulation_output.generated_content.append(content_gen)
        
        # Auto-save generated content to organized folders
        try:
            saved_content_files = save_content_automatically(simulation_output.to_dict())
            if saved_content_files:
                console.print(f"[green]Auto-saved {len(saved_content_files)} content item(s) to generated_content/[/green]")
                for file_path in saved_content_files:
                    console.print(f"   → {file_path}")
        except Exception as e:
            console.print(f"[yellow]Auto-save warning: {str(e)}[/yellow]")
            import logging
            logger_warn = logging.getLogger(__name__)
            logger_warn.warning(f"Auto content save failed: {e}")
        
        # Save to file using simulation logger
        logger = SimulationLogger()
        saved_file = logger.save_simulation_result(simulation_output, OutputFormat.JSON)
        
        # Prepare results dictionary for output formatting
        results = simulation_output.to_dict()
        
    except Exception as e:
        console.print(f"[red]Simulation failed: {str(e)}[/red]")
        import logging
        logger = logging.getLogger(__name__)
        logger.error(f"Simulation execution failed: {e}")
        return
    
    # Output results in requested format
    if output == "json":
        results_json = json.dumps(results, indent=2, default=str)
        console.print(f"\n[bold blue]JSON Results:[/bold blue]")
        from rich.syntax import Syntax
        syntax = Syntax(results_json, "json", theme="monokai", line_numbers=True)
        console.print(Panel(syntax, title="Simulation Results (JSON)", border_style="blue"))
    
    elif output == "yaml":
        import yaml
        results_yaml = yaml.dump(results, default_flow_style=False, sort_keys=False)
        console.print(f"\n[bold blue]YAML Results:[/bold blue]")
        from rich.syntax import Syntax
        syntax = Syntax(results_yaml, "yaml", theme="monokai", line_numbers=True)
        console.print(Panel(syntax, title="Simulation Results (YAML)", border_style="blue"))
    
    else:  # report format
        console.print(f"\n[bold blue]Simulation Report:[/bold blue]")
        
        report_table = Table(title=f"Threat Simulation Report - {results['simulation_id']}", show_header=True)
        report_table.add_column("Metric", style="cyan")
        report_table.add_column("Value", style="white")
        report_table.add_column("Status", justify="center")
        
        report_table.add_row("Content Generation", "Successful", "[green]PASS[/green]")
        report_table.add_row("Delivery Method", delivery_vector_val.replace('_', ' ').title(), "[green]PASS[/green]")
        report_table.add_row("Target Engagement", "High", "[green]PASS[/green]")
        report_table.add_row("Security Response", "Appropriate", "[green]PASS[/green]")
        report_table.add_row("Learning Objectives", "Met", "[green]PASS[/green]")
        
        console.print(report_table)
        
        console.print("\n[bold yellow]Recommendations:[/bold yellow]")
        for i, rec in enumerate(results["recommendations"], 1):
            console.print(f"  {i}. {rec}")
    
    console.print(f"\n[dim]Full results saved to: {saved_file}[/dim]")
    console.print(f"[dim]Results can be loaded using simulation ID: {results['simulation_id']}[/dim]")
    console.print("[green]Threat simulation completed successfully![/green]")


@cli.command()
@click.argument('config_file', required=False)
@click.option(
    "--schema-only",
    is_flag=True,
    help="Validate schema only, skip semantic validation",
)
@click.option(
    "--directory", 
    "-d",
    help="Validate all YAML files in directory",
)
@click.option(
    "--strict",
    is_flag=True,
    help="Enable strict validation mode",
)
def validate(config_file: Optional[str], schema_only: bool, directory: Optional[str], strict: bool) -> None:
    """Validate threat scenario configurations against the YAML schema."""
    from ..config.yaml_loader import YAMLConfigLoader, ConfigurationError, SchemaValidationError
    
    loader = YAMLConfigLoader(strict_mode=strict, schema_validation=not schema_only)
    
    if directory:
        # Validate all files in directory
        dir_path = Path(directory)
        console.print(f"[blue]Validating all YAML files in:[/blue] {dir_path}")
        
        try:
            results = loader.validate_config_directory(dir_path)
            
            # Display summary
            total = results['total_files']
            valid = results['valid_files']
            invalid = results['invalid_files']
            
            console.print(f"\n[bold cyan]Validation Summary[/bold cyan]")
            console.print(f"Total Files: {total}")
            console.print(f"Valid: [green]{valid}[/green]")
            console.print(f"Invalid: [red]{invalid}[/red]")
            
            if total > 0:
                success_rate = (valid / total) * 100
                if success_rate == 100:
                    console.print(f"Success Rate: [green]{success_rate:.1f}%[/green]")
                elif success_rate >= 80:
                    console.print(f"Success Rate: [yellow]{success_rate:.1f}%[/yellow]")
                else:
                    console.print(f"Success Rate: [red]{success_rate:.1f}%[/red]")
            
            # Show failed files
            if invalid > 0:
                console.print(f"\n[bold red]Failed Validations:[/bold red]")
                for file_path, file_result in results['files'].items():
                    if file_result['status'] == 'invalid':
                        console.print(f"  • {file_path}: {file_result['error'][:80]}...")
            
        except ConfigurationError as e:
            console.print(f"[red]Directory validation error:[/red] {e}")
            return
    
    elif config_file:
        # Validate single file
        file_path = Path(config_file)
        console.print(f"[blue]Validating configuration:[/blue] {file_path}")
        
        if schema_only:
            console.print("[dim]Schema validation only (semantic validation disabled)[/dim]")
        
        try:
            # Load and validate
            config = loader.load_config(file_path)
            console.print("[green]Configuration loaded successfully[/green]")
            
            if not schema_only:
                scenario = loader.validate_threat_scenario(config)
                console.print("[green]Schema validation passed[/green]")
                
                # Display scenario summary
                metadata = scenario.metadata
                console.print(f"\n[bold cyan]Scenario Summary:[/bold cyan]")
                console.print(f"Name: [cyan]{metadata.name}[/cyan]")
                threat_type_val = scenario.threat_type.value if hasattr(scenario.threat_type, 'value') else str(scenario.threat_type)
                difficulty_val = scenario.difficulty_level.value if hasattr(scenario.difficulty_level, 'value') else scenario.difficulty_level
                console.print(f"Threat Type: [yellow]{threat_type_val.replace('_', ' ').title()}[/yellow]")
                console.print(f"Difficulty: [magenta]{difficulty_val}/10[/magenta]")
                console.print(f"Duration: [blue]{scenario.estimated_duration} minutes[/blue]")
                seniority_val = scenario.target_profile.seniority.value if hasattr(scenario.target_profile.seniority, 'value') else str(scenario.target_profile.seniority)
                console.print(f"Target: [green]{scenario.target_profile.role} ({seniority_val.replace('_', ' ').title()})[/green]")
            
        except ConfigurationError as e:
            console.print(f"[red]Configuration Error:[/red] {e}")
        except SchemaValidationError as e:
            console.print(f"[red]Schema Validation Failed:[/red] {e}")
            if e.errors:
                console.print("\n[bold red]Validation Errors:[/bold red]")
                for i, error in enumerate(e.errors[:10], 1):  # Show first 10 errors
                    console.print(f"  {i}. [red]{error['location']}:[/red] {error['message']}")
                if len(e.errors) > 10:
                    console.print(f"  ... and {len(e.errors) - 10} more errors")
    
    else:
        # Validate current directory by default
        current_dir = Path.cwd()
        console.print(f"[blue]Validating YAML files in current directory:[/blue] {current_dir}")
        
        yaml_files = list(current_dir.glob("*.yaml")) + list(current_dir.glob("*.yml"))
        
        if not yaml_files:
            console.print("[yellow]No YAML files found in current directory[/yellow]")
            return
        
        valid_count = 0
        for yaml_file in yaml_files:
            try:
                config = loader.load_config(yaml_file)
                if not schema_only:
                    scenario = loader.validate_threat_scenario(config)
                console.print(f"[green]PASS[/green] {yaml_file.name}")
                valid_count += 1
            except (ConfigurationError, SchemaValidationError) as e:
                console.print(f"[red]FAIL[/red] {yaml_file.name}: {str(e)[:60]}...")
        
        console.print(f"\n[cyan]Validated {valid_count}/{len(yaml_files)} files successfully[/cyan]")


# Add templates command group
cli.add_command(templates)


@cli.command()
@click.option(
    "--simulation-id",
    required=True,
    help="Simulation ID to generate report for",
)
@click.option(
    "--format",
    type=click.Choice(["pdf", "html", "json", "csv"]),
    default="html",
    help="Report format",
)
@click.option(
    "--output-file",
    "-o",
    help="Output file path",
)
def report(simulation_id: str, format: str, output_file: Optional[str]) -> None:
    """Generate simulation reports."""
    console.print(f"[blue]Generating {format} report for simulation:[/blue] {simulation_id}")
    
    if output_file:
        console.print(f"[dim]Output file: {output_file}[/dim]")
    
    # TODO: Implement report generation
    console.print("[yellow]Report generation feature coming soon![/yellow]")
    console.print("This will be implemented in Week 11 (Analytics & Reporting)")


@cli.command()
@click.option(
    "--check-content",
    help="Check content against safety policies",
)
@click.option(
    "--policy",
    help="Specific policy to check against",
)
def safety(check_content: Optional[str], policy: Optional[str]) -> None:
    """Safety and compliance checking tools."""
    if check_content:
        console.print(f"[blue]Checking content safety:[/blue] {check_content[:50]}...")
    else:
        console.print("[blue]Safety policy information[/blue]")
    
    # TODO: Implement safety checking
    console.print("[yellow]Safety checking feature coming soon![/yellow]")
    console.print("This will be implemented in Week 9 (Safety & Ethics Framework)")


@cli.command()
def status() -> None:
    """Show ThreatGPT system status."""
    console.print("[blue]ThreatGPT System Status[/blue]")
    
    status_table = Table(title="Component Status")
    status_table.add_column("Component", style="cyan")
    status_table.add_column("Status", style="green")
    status_table.add_column("Version", style="white")
    
    # TODO: Add actual status checks
    status_table.add_row("CLI Interface", "Active", __version__)
    status_table.add_row("Configuration Engine", "Active", "v2.0")
    status_table.add_row("LLM Integration", "Active", "v1.0")
    status_table.add_row("Simulation Engine", "Active", "v1.0")
    status_table.add_row("API Gateway", "In Development", "N/A")
    status_table.add_row("Safety Module", "In Development", "N/A")
    
    console.print(status_table)


# Add LLM integration commands
cli.add_command(llm_group, name="llm")

# Add intelligence commands
from .intelligence import intel_group
cli.add_command(intel_group, name="intel")

# Add deployment commands
from .deploy import deploy_group
cli.add_command(deploy_group, name="deploy")

# Add dataset commands
from .datasets import datasets
cli.add_command(datasets, name="datasets")

# Add logs management commands
cli.add_command(logs_group, name="logs")


def main() -> int:
    """Main entry point for the CLI."""
    try:
        cli()
        return 0
    except KeyboardInterrupt:
        console.print("\n[red]Operation cancelled by user[/red]")
        return 1
    except Exception as e:
        console.print(f"[red]Error: {e}[/red]")
        return 1


if __name__ == "__main__":
    sys.exit(main())