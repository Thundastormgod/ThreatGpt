"""Provider abstraction layer for LLM integrations.

This module provides a unified interface for multiple LLM providers,
handling authentication, rate limiting, retries, and response normalization.
"""

import asyncio
import logging
import time
from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional

import httpx
from pydantic import ValidationError

from .models import (
    ContentType,
    GenerationResult,
    LLMModel,
    LLMProvider,
    LLMProviderConfig,
    LLMRequest,
    LLMResponse,
    PromptContext,
)

logger = logging.getLogger(__name__)


class RateLimiter:
    """Token bucket rate limiter for API requests."""
    
    def __init__(self, max_requests: int, max_tokens: int, window_seconds: int = 60):
        self.max_requests = max_requests
        self.max_tokens = max_tokens
        self.window_seconds = window_seconds
        
        self.request_timestamps = []
        self.token_timestamps = []
        self.token_counts = []
    
    async def acquire(self, estimated_tokens: int = 1000) -> bool:
        """Acquire rate limit permission for a request."""
        now = time.time()
        
        # Clean old timestamps
        cutoff = now - self.window_seconds
        self.request_timestamps = [t for t in self.request_timestamps if t > cutoff]
        
        # Clean old token usage
        while self.token_timestamps and self.token_timestamps[0] <= cutoff:
            self.token_timestamps.pop(0)
            self.token_counts.pop(0)
        
        # Check request limit
        if len(self.request_timestamps) >= self.max_requests:
            return False
        
        # Check token limit
        current_tokens = sum(self.token_counts)
        if current_tokens + estimated_tokens > self.max_tokens:
            return False
        
        # Record this request
        self.request_timestamps.append(now)
        self.token_timestamps.append(now)
        self.token_counts.append(estimated_tokens)
        
        return True


class BaseLLMProvider(ABC):
    """Abstract base class for LLM providers."""
    
    def __init__(self, config: LLMProviderConfig):
        self.config = config
        self.rate_limiter = RateLimiter(
            max_requests=config.max_requests_per_minute,
            max_tokens=config.max_tokens_per_minute
        )
        self._client: Optional[httpx.AsyncClient] = None
    
    @property
    def client(self) -> httpx.AsyncClient:
        """Get or create HTTP client."""
        if self._client is None:
            self._client = httpx.AsyncClient(timeout=30.0)
        return self._client
    
    async def close(self):
        """Close the HTTP client."""
        if self._client:
            await self._client.aclose()
            self._client = None
    
    @abstractmethod
    async def generate(self, request: LLMRequest) -> LLMResponse:
        """Generate content using the provider's API."""
        pass
    
    @abstractmethod
    def estimate_tokens(self, text: str) -> int:
        """Estimate token count for text."""
        pass
    
    @abstractmethod
    def validate_model(self, model: LLMModel) -> bool:
        """Check if model is supported by this provider."""
        pass
    
    async def generate_with_retry(self, request: LLMRequest) -> LLMResponse:
        """Generate content with automatic retries and rate limiting."""
        estimated_tokens = self.estimate_tokens(request.user_prompt)
        if request.system_prompt:
            estimated_tokens += self.estimate_tokens(request.system_prompt)
        
        # Rate limiting
        max_wait = 60  # Maximum wait time in seconds
        wait_time = 1
        
        while wait_time <= max_wait:
            if await self.rate_limiter.acquire(estimated_tokens):
                break
            logger.warning(f"Rate limit hit, waiting {wait_time}s")
            await asyncio.sleep(wait_time)
            wait_time = min(wait_time * 2, max_wait)
        else:
            raise RuntimeError("Rate limit exceeded, max wait time reached")
        
        # Retry logic
        last_exception = None
        
        for attempt in range(self.config.max_retries + 1):
            try:
                return await self.generate(request)
            except Exception as e:
                last_exception = e
                if attempt < self.config.max_retries:
                    delay = self.config.retry_delay_seconds * (2 ** attempt)
                    logger.warning(f"Request failed (attempt {attempt + 1}), retrying in {delay}s: {e}")
                    await asyncio.sleep(delay)
                else:
                    logger.error(f"Request failed after {self.config.max_retries + 1} attempts: {e}")
        
        # Return error response
        return LLMResponse(
            request_id=request.request_id,
            provider=request.provider,
            model=request.model,
            content="",
            content_type=request.content_type,
            prompt_tokens=0,
            completion_tokens=0,
            total_tokens=0,
            response_time_ms=0,
            error=str(last_exception),
            error_code="generation_failed"
        )


class OpenAIProvider(BaseLLMProvider):
    """OpenAI API provider implementation."""
    
    SUPPORTED_MODELS = {
        LLMModel.GPT_4_TURBO,
        LLMModel.GPT_4,
        LLMModel.GPT_35_TURBO,
    }
    
    def __init__(self, config: LLMProviderConfig):
        super().__init__(config)
        self.base_url = config.base_url or "https://api.openai.com/v1"
    
    def validate_model(self, model: LLMModel) -> bool:
        """Check if model is supported by OpenAI."""
        return model in self.SUPPORTED_MODELS
    
    def estimate_tokens(self, text: str) -> int:
        """Rough token estimation (1 token â‰ˆ 4 characters for English)."""
        return max(1, len(text) // 4)
    
    async def generate(self, request: LLMRequest) -> LLMResponse:
        """Generate content using OpenAI API."""
        if not self.validate_model(request.model):
            raise ValueError(f"Model {request.model} not supported by OpenAI provider")
        
        start_time = time.time()
        
        # Prepare request payload
        messages = []
        if request.system_prompt:
            messages.append({"role": "system", "content": request.system_prompt})
        messages.append({"role": "user", "content": request.user_prompt})
        
        payload = {
            "model": request.model.value,
            "messages": messages,
            "temperature": request.temperature,
            "max_tokens": request.max_tokens,
            "top_p": request.top_p,
            "frequency_penalty": request.frequency_penalty,
            "presence_penalty": request.presence_penalty,
        }
        
        headers = {
            "Authorization": f"Bearer {self.config.api_key}",
            "Content-Type": "application/json",
        }
        
        if self.config.organization:
            headers["OpenAI-Organization"] = self.config.organization
        
        try:
            response = await self.client.post(
                f"{self.base_url}/chat/completions",
                json=payload,
                headers=headers,
                timeout=request.timeout_seconds
            )
            response.raise_for_status()
            
            data = response.json()
            response_time = int((time.time() - start_time) * 1000)
            
            # Extract response data
            choice = data["choices"][0]
            usage = data["usage"]
            
            return LLMResponse(
                request_id=request.request_id,
                provider=request.provider,
                model=request.model,
                content=choice["message"]["content"],
                content_type=request.content_type,
                prompt_tokens=usage["prompt_tokens"],
                completion_tokens=usage["completion_tokens"],
                total_tokens=usage["total_tokens"],
                response_time_ms=response_time,
                finish_reason=choice.get("finish_reason"),
                raw_response=data
            )
            
        except httpx.HTTPStatusError as e:
            error_msg = f"OpenAI API error: {e.response.status_code}"
            try:
                error_data = e.response.json()
                error_msg += f" - {error_data.get('error', {}).get('message', 'Unknown error')}"
            except:
                error_msg += f" - {e.response.text}"
            
            return LLMResponse(
                request_id=request.request_id,
                provider=request.provider,
                model=request.model,
                content="",
                content_type=request.content_type,
                prompt_tokens=0,
                completion_tokens=0,
                total_tokens=0,
                response_time_ms=int((time.time() - start_time) * 1000),
                error=error_msg,
                error_code=str(e.response.status_code)
            )
        
        except Exception as e:
            return LLMResponse(
                request_id=request.request_id,
                provider=request.provider,
                model=request.model,
                content="",
                content_type=request.content_type,
                prompt_tokens=0,
                completion_tokens=0,
                total_tokens=0,
                response_time_ms=int((time.time() - start_time) * 1000),
                error=str(e),
                error_code="request_failed"
            )


class AnthropicProvider(BaseLLMProvider):
    """Anthropic Claude API provider implementation."""
    
    SUPPORTED_MODELS = {
        LLMModel.CLAUDE_3_SONNET,
        LLMModel.CLAUDE_3_HAIKU,
        LLMModel.CLAUDE_2_1,
        LLMModel.CLAUDE_2,
    }
    
    def __init__(self, config: LLMProviderConfig):
        super().__init__(config)
        self.base_url = config.base_url or "https://api.anthropic.com"
    
    def validate_model(self, model: LLMModel) -> bool:
        """Check if model is supported by Anthropic."""
        return model in self.SUPPORTED_MODELS
    
    def estimate_tokens(self, text: str) -> int:
        """Rough token estimation for Claude."""
        return max(1, len(text) // 3)  # Claude tends to have slightly better compression
    
    async def generate(self, request: LLMRequest) -> LLMResponse:
        """Generate content using Anthropic API."""
        if not self.validate_model(request.model):
            raise ValueError(f"Model {request.model} not supported by Anthropic provider")
        
        start_time = time.time()
        
        # Prepare request payload
        payload = {
            "model": request.model.value,
            "max_tokens": request.max_tokens,
            "temperature": request.temperature,
            "top_p": request.top_p,
            "messages": [
                {"role": "user", "content": request.user_prompt}
            ]
        }
        
        # Add system prompt if provided
        if request.system_prompt:
            payload["system"] = request.system_prompt
        
        headers = {
            "x-api-key": self.config.api_key,
            "Content-Type": "application/json",
            "anthropic-version": "2023-06-01",
        }
        
        try:
            response = await self.client.post(
                f"{self.base_url}/v1/messages",
                json=payload,
                headers=headers,
                timeout=request.timeout_seconds
            )
            response.raise_for_status()
            
            data = response.json()
            response_time = int((time.time() - start_time) * 1000)
            
            # Extract response data
            content = data["content"][0]["text"] if data["content"] else ""
            usage = data["usage"]
            
            return LLMResponse(
                request_id=request.request_id,
                provider=request.provider,
                model=request.model,
                content=content,
                content_type=request.content_type,
                prompt_tokens=usage["input_tokens"],
                completion_tokens=usage["output_tokens"],
                total_tokens=usage["input_tokens"] + usage["output_tokens"],
                response_time_ms=response_time,
                finish_reason=data.get("stop_reason"),
                raw_response=data
            )
            
        except httpx.HTTPStatusError as e:
            error_msg = f"Anthropic API error: {e.response.status_code}"
            try:
                error_data = e.response.json()
                error_msg += f" - {error_data.get('error', {}).get('message', 'Unknown error')}"
            except:
                error_msg += f" - {e.response.text}"
            
            return LLMResponse(
                request_id=request.request_id,
                provider=request.provider,
                model=request.model,
                content="",
                content_type=request.content_type,
                prompt_tokens=0,
                completion_tokens=0,
                total_tokens=0,
                response_time_ms=int((time.time() - start_time) * 1000),
                error=error_msg,
                error_code=str(e.response.status_code)
            )
        
        except Exception as e:
            return LLMResponse(
                request_id=request.request_id,
                provider=request.provider,
                model=request.model,
                content="",
                content_type=request.content_type,
                prompt_tokens=0,
                completion_tokens=0,
                total_tokens=0,
                response_time_ms=int((time.time() - start_time) * 1000),
                error=str(e),
                error_code="request_failed"
            )


class OpenRouterProviderAdapter(BaseLLMProvider):
    """Adapter for OpenRouter provider to work with the new provider interface."""
    
    def __init__(self, config: LLMProviderConfig):
        super().__init__(config)
        self.api_key = config.api_key
        self.model = config.default_model.value
        self.base_url = "https://openrouter.ai/api/v1"
    
    async def generate(self, request: LLMRequest) -> LLMResponse:
        """Generate content using OpenRouter API."""
        if not self.api_key:
            return LLMResponse(
                content="",
                provider=self.config.provider.value,
                model=self.model,
                prompt_tokens=0,
                completion_tokens=0,
                total_tokens=0,
                response_time_ms=0,
                error="OpenRouter API key not configured",
                error_code="missing_api_key"
            )
        
        start_time = time.time()
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "HTTP-Referer": "https://github.com/threatgpt/threatgpt",
            "X-Title": "ThreatGPT",
            "Content-Type": "application/json"
        }
        
        messages = []
        if request.system_prompt:
            messages.append({"role": "system", "content": request.system_prompt})
        messages.append({"role": "user", "content": request.user_prompt})
        
        payload = {
            "model": self.model,
            "messages": messages,
            "max_tokens": request.max_tokens,
            "temperature": request.temperature
        }
        
        try:
            response = await self.client.post(
                f"{self.base_url}/chat/completions",
                json=payload,
                headers=headers
            )
            
            if response.status_code == 200:
                data = response.json()
                choice = data["choices"][0]
                usage = data.get("usage", {})
                
                return LLMResponse(
                    content=choice["message"]["content"],
                    provider=self.config.provider.value,
                    model=self.model,
                    prompt_tokens=usage.get("prompt_tokens", 0),
                    completion_tokens=usage.get("completion_tokens", 0),
                    total_tokens=usage.get("total_tokens", 0),
                    response_time_ms=int((time.time() - start_time) * 1000)
                )
            else:
                error_data = response.json() if response.headers.get("content-type", "").startswith("application/json") else {}
                error_msg = error_data.get("error", {}).get("message", f"HTTP {response.status_code}")
                
                return LLMResponse(
                    content="",
                    provider=self.config.provider.value,
                    model=self.model,
                    prompt_tokens=0,
                    completion_tokens=0,
                    total_tokens=0,
                    response_time_ms=int((time.time() - start_time) * 1000),
                    error=error_msg,
                    error_code=f"http_{response.status_code}"
                )
                
        except Exception as e:
            return LLMResponse(
                content="",
                provider=self.config.provider.value,
                model=self.model,
                prompt_tokens=0,
                completion_tokens=0,
                total_tokens=0,
                response_time_ms=int((time.time() - start_time) * 1000),
                error=str(e),
                error_code="request_failed"
            )
    
    def estimate_tokens(self, text: str) -> int:
        """Estimate token count for text."""
        # Simple estimation: ~4 characters per token
        return len(text) // 4
    
    def validate_model(self, model: LLMModel) -> bool:
        """Check if model is supported by OpenRouter."""
        # OpenRouter supports many models, so we'll accept most common ones
        return True


class LLMProviderManager:
    """Manager for multiple LLM providers with automatic routing."""
    
    def __init__(self):
        self.providers: Dict[LLMProvider, BaseLLMProvider] = {}
        self.default_provider: Optional[LLMProvider] = None
    
    def add_provider(self, config: LLMProviderConfig, set_as_default: bool = False):
        """Add a provider configuration."""
        if config.provider == LLMProvider.OPENAI:
            provider = OpenAIProvider(config)
        elif config.provider == LLMProvider.ANTHROPIC:
            provider = AnthropicProvider(config)
        elif config.provider == LLMProvider.OPENROUTER:
            # Create a simple OpenRouter provider for testing
            provider = OpenRouterProviderAdapter(config)
        else:
            raise ValueError(f"Unsupported provider: {config.provider}")
        
        self.providers[config.provider] = provider
        
        if set_as_default or self.default_provider is None:
            self.default_provider = config.provider
    
    def get_provider(self, provider: Optional[LLMProvider] = None) -> BaseLLMProvider:
        """Get a specific provider or the default one."""
        target_provider = provider or self.default_provider
        
        if target_provider is None:
            raise ValueError("No provider specified and no default provider set")
        
        if target_provider not in self.providers:
            raise ValueError(f"Provider {target_provider} not configured")
        
        return self.providers[target_provider]
    
    async def generate(self, request: LLMRequest) -> LLMResponse:
        """Generate content using the appropriate provider."""
        provider = self.get_provider(request.provider)
        return await provider.generate_with_retry(request)
    
    async def close_all(self):
        """Close all provider connections."""
        for provider in self.providers.values():
            await provider.close()
    
    def list_providers(self) -> List[str]:
        """List configured providers."""
        return [p.value for p in self.providers.keys()]
    
    def get_supported_models(self, provider: Optional[LLMProvider] = None) -> List[LLMModel]:
        """Get supported models for a provider."""
        target_provider = provider or self.default_provider
        
        if target_provider == LLMProvider.OPENAI:
            return list(OpenAIProvider.SUPPORTED_MODELS)
        elif target_provider == LLMProvider.ANTHROPIC:
            return list(AnthropicProvider.SUPPORTED_MODELS)
        elif target_provider == LLMProvider.OPENROUTER:
            # OpenRouter supports many models - return common ones
            return [LLMModel.GPT_35_TURBO, LLMModel.GPT_4, LLMModel.CLAUDE_3_HAIKU]
        else:
            return []